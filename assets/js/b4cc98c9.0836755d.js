"use strict";(self.webpackChunkdiscoverse_docs=self.webpackChunkdiscoverse_docs||[]).push([[312],{8453:(i,e,n)=>{n.d(e,{R:()=>a,x:()=>s});var t=n(6540);const o={},r=t.createContext(o);function a(i){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function s(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(o):i.components||o:a(i.components),t.createElement(r.Provider,{value:e},i.children)}},9699:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"tutorials/imitation-learning/dp","title":"Diffusion Policy","description":"Diffusion Policy is an imitation learning algorithm based on diffusion models.","source":"@site/docs/tutorials/imitation-learning/dp.md","sourceDirName":"tutorials/imitation-learning","slug":"/tutorials/imitation-learning/dp","permalink":"/docs/tutorials/imitation-learning/dp","draft":false,"unlisted":false,"editUrl":"https://github.com/TATP-233/DISCOVERSE/tree/main/discoverse-docs/docs/tutorials/imitation-learning/dp.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"ACT (Action Chunking with Transformers)","permalink":"/docs/tutorials/imitation-learning/act"},"next":{"title":"RDT (Robotics Diffusion Transformer)","permalink":"/docs/tutorials/imitation-learning/rdt"}}');var o=n(4848),r=n(8453);const a={sidebar_position:4},s="Diffusion Policy",d={},l=[{value:"\ud83d\udcca Data Format Conversion",id:"-data-format-conversion",level:2},{value:"Dependency Installation",id:"dependency-installation",level:3},{value:"Conversion Command",id:"conversion-command",level:3},{value:"\ud83c\udf93 Model Training",id:"-model-training",level:2},{value:"Training Configuration",id:"training-configuration",level:3}];function c(i){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...i.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"diffusion-policy",children:"Diffusion Policy"})}),"\n",(0,o.jsx)(e.p,{children:"Diffusion Policy is an imitation learning algorithm based on diffusion models."}),"\n",(0,o.jsx)(e.p,{children:"In this repository, both the dp and Diffusion-Policy modules implement the Diffusion Policy algorithm, but they come from different codebases and may differ in engineering structure or implementation details."}),"\n",(0,o.jsx)(e.h2,{id:"-data-format-conversion",children:"\ud83d\udcca Data Format Conversion"}),"\n",(0,o.jsx)(e.h3,{id:"dependency-installation",children:"Dependency Installation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip install -r policies/dp/requirements.txt\n"})}),"\n",(0,o.jsx)(e.h3,{id:"conversion-command",children:"Conversion Command"}),"\n",(0,o.jsx)(e.p,{children:"Convert raw simulation data to the Zarr format required by the Diffusion Policy algorithm:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"python3 policies/dp/raw2zarr.py -dir data -tn <task_name>\n"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"-dir"}),": Root directory for data storage, default is data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"-tn"}),": Task name, the program will look for a dataset folder with the same name in the data directory"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:["The converted data will be stored in the ",(0,o.jsx)(e.code,{children:"discoverse/data/zarr"})," folder."]}),"\n",(0,o.jsx)(e.h1,{id:"dp",children:"dp"}),"\n",(0,o.jsx)(e.h2,{id:"-model-training",children:"\ud83c\udf93 Model Training"}),"\n",(0,o.jsx)(e.h3,{id:"training-configuration",children:"Training Configuration"}),"\n",(0,o.jsxs)(e.p,{children:["The reference training configuration file is located at ",(0,o.jsx)(e.code,{children:"policies/dp/configs/block_place.yaml"}),", with the main parameters explained as follows:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"task_path"}),": During inference, the program loads the ",(0,o.jsx)(e.code,{children:"SimNode"})," class and instance ",(0,o.jsx)(e.code,{children:"cfg"})," to create the simulation environment"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"max_episode_steps"}),": Total number of action steps during inference"]}),"\n"]})]})}function h(i={}){const{wrapper:e}={...(0,r.R)(),...i.components};return e?(0,o.jsx)(e,{...i,children:(0,o.jsx)(c,{...i})}):c(i)}}}]);