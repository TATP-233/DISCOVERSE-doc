# æœºå™¨äººæ§åˆ¶

æœ¬æ•™ç¨‹å°†æ·±å…¥ä»‹ç»DISCOVERSEä¸­çš„æœºå™¨äººæ§åˆ¶ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸åŒçš„æ§åˆ¶æ¨¡å¼ã€è¿åŠ¨å­¦è®¡ç®—ã€è½¨è¿¹è§„åˆ’å’Œå®æ—¶æ§åˆ¶ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡DISCOVERSEçš„æ‰€æœ‰æ§åˆ¶æ¨¡å¼
- ç†è§£æ­£å‘å’Œåå‘è¿åŠ¨å­¦
- å­¦ä¹ è½¨è¿¹è§„åˆ’å’Œå¹³æ»‘æ§åˆ¶
- å®ç°å¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡

## ğŸ® æ§åˆ¶æ¨¡å¼è¯¦è§£

### 1. å…³èŠ‚ä½ç½®æ§åˆ¶ (pd_joint_pos)

æœ€åŸºç¡€çš„æ§åˆ¶æ¨¡å¼ï¼Œç›´æ¥æŒ‡å®šç›®æ ‡å…³èŠ‚è§’åº¦ï¼š

```python
import discoverse as dv
import numpy as np

env = dv.make_env("airbot_play", control_mode="pd_joint_pos")
obs = env.reset()

# è·å–å½“å‰å…³èŠ‚ä½ç½®
current_qpos = obs["qpos"]  # å½“å‰å…³èŠ‚è§’åº¦
print(f"å½“å‰å…³èŠ‚ä½ç½®: {current_qpos}")

# è®¾ç½®ç›®æ ‡å…³èŠ‚ä½ç½®
target_qpos = np.array([0.0, -0.5, 0.0, 1.57, 0.0, 1.0, 0.0])

# æ‰§è¡Œæ§åˆ¶
obs, reward, done, info = env.step(target_qpos)
env.render()
```

### 2. å…³èŠ‚å¢é‡æ§åˆ¶ (pd_joint_delta_pos)

é€šè¿‡å¢é‡æ–¹å¼æ§åˆ¶å…³èŠ‚ï¼Œé€‚åˆç²¾ç»†æ“ä½œï¼š

```python
env = dv.make_env("airbot_play", control_mode="pd_joint_delta_pos")
obs = env.reset()

# å¹³æ»‘ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
target = np.array([0.0, -0.5, 0.0, 1.57, 0.0, 1.0, 0.0])
current = obs["qpos"]

for step in range(100):
    # è®¡ç®—å¢é‡
    delta = (target - current) * 0.1  # 10%çš„æ­¥é•¿
    
    # æ‰§è¡Œå¢é‡æ§åˆ¶
    obs, reward, done, info = env.step(delta)
    current = obs["qpos"]
    
    # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
    if np.linalg.norm(target - current) < 0.01:
        print(f"åœ¨ç¬¬{step}æ­¥åˆ°è¾¾ç›®æ ‡")
        break
    
    env.render()
```

### 3. æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶ (pd_ee_pose)

ç›´æ¥æ§åˆ¶æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨çš„ä½å§¿ï¼š

```python
env = dv.make_env("airbot_play", control_mode="pd_ee_pose")
obs = env.reset()

# è·å–å½“å‰æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿
current_ee_pose = obs["ee_pose"]  # [x, y, z, qx, qy, qz, qw]
print(f"å½“å‰æœ«ç«¯ä½å§¿: {current_ee_pose}")

# è®¾ç½®ç›®æ ‡ä½å§¿ (ä½ç½® + å››å…ƒæ•°)
target_position = [0.5, 0.2, 0.3]  # x, y, z
target_quaternion = [0.0, 0.0, 0.0, 1.0]  # qx, qy, qz, qw (æ— æ—‹è½¬)
target_pose = target_position + target_quaternion

# æ‰§è¡Œæ§åˆ¶
obs, reward, done, info = env.step(target_pose)
env.render()
```

### 4. é€Ÿåº¦æ§åˆ¶ (pd_joint_vel)

æ§åˆ¶å…³èŠ‚é€Ÿåº¦ï¼š

```python
env = dv.make_env("airbot_play", control_mode="pd_joint_vel")
obs = env.reset()

# è®¾ç½®å…³èŠ‚é€Ÿåº¦ (rad/s)
joint_velocities = np.array([0.1, -0.2, 0.0, 0.3, 0.0, -0.1, 0.0])

for step in range(100):
    obs, reward, done, info = env.step(joint_velocities)
    
    # å¯ä»¥åŠ¨æ€è°ƒæ•´é€Ÿåº¦
    if step == 50:
        joint_velocities *= -1  # åå‘è¿åŠ¨
    
    env.render()
```

## ğŸ”§ è¿åŠ¨å­¦è®¡ç®—

### æ­£å‘è¿åŠ¨å­¦ (Forward Kinematics)

ä»å…³èŠ‚è§’åº¦è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ï¼š

```python
import discoverse as dv

env = dv.make_env("airbot_play")
robot = env.robot  # è·å–æœºå™¨äººå¯¹è±¡

# ç»™å®šå…³èŠ‚è§’åº¦
joint_angles = np.array([0.0, -0.5, 0.0, 1.57, 0.0, 1.0, 0.0])

# è®¡ç®—æ­£å‘è¿åŠ¨å­¦
ee_pose = robot.forward_kinematics(joint_angles)
print(f"æœ«ç«¯ä½ç½®: {ee_pose[:3]}")
print(f"æœ«ç«¯æ–¹å‘ (å››å…ƒæ•°): {ee_pose[3:]}")

# ä¹Ÿå¯ä»¥è®¡ç®—é›…å¯æ¯”çŸ©é˜µ
jacobian = robot.compute_jacobian(joint_angles)
print(f"é›…å¯æ¯”çŸ©é˜µå½¢çŠ¶: {jacobian.shape}")  # (6, 7) for 7-DOF arm
```

### åå‘è¿åŠ¨å­¦ (Inverse Kinematics)

ä»ç›®æ ‡æœ«ç«¯ä½å§¿è®¡ç®—å…³èŠ‚è§’åº¦ï¼š

```python
# ç›®æ ‡æœ«ç«¯ä½å§¿
target_position = [0.5, 0.0, 0.3]
target_orientation = [0.0, 0.0, 0.0, 1.0]  # å››å…ƒæ•°
target_pose = np.array(target_position + target_orientation)

# è®¡ç®—åå‘è¿åŠ¨å­¦
ik_solution = robot.inverse_kinematics(
    target_pose,
    initial_guess=joint_angles,  # åˆå§‹çŒœæµ‹
    tolerance=1e-4,              # æ±‚è§£ç²¾åº¦
    max_iterations=100           # æœ€å¤§è¿­ä»£æ¬¡æ•°
)

if ik_solution is not None:
    print(f"IKè§£: {ik_solution}")
    
    # éªŒè¯è§£çš„æ­£ç¡®æ€§
    computed_pose = robot.forward_kinematics(ik_solution)
    error = np.linalg.norm(computed_pose - target_pose)
    print(f"ä½å§¿è¯¯å·®: {error:.6f}")
else:
    print("æœªæ‰¾åˆ°IKè§£")
```

### å¤šè§£å¤„ç†

IKé—®é¢˜å¯èƒ½æœ‰å¤šä¸ªè§£ï¼ŒDISCOVERSEæä¾›äº†å¤„ç†æ–¹æ³•ï¼š

```python
# è·å–æ‰€æœ‰å¯èƒ½çš„IKè§£
all_solutions = robot.inverse_kinematics_all(
    target_pose,
    tolerance=1e-4
)

print(f"æ‰¾åˆ° {len(all_solutions)} ä¸ªè§£")

# é€‰æ‹©æœ€æ¥è¿‘å½“å‰å…³èŠ‚è§’åº¦çš„è§£
current_angles = obs["qpos"]
best_solution = None
min_distance = float('inf')

for solution in all_solutions:
    distance = np.linalg.norm(solution - current_angles)
    if distance < min_distance:
        min_distance = distance
        best_solution = solution

print(f"æœ€ä¼˜è§£: {best_solution}")
```

## ğŸ“ˆ è½¨è¿¹è§„åˆ’

### å…³èŠ‚ç©ºé—´è½¨è¿¹

åœ¨å…³èŠ‚ç©ºé—´ä¸­è§„åˆ’å¹³æ»‘è½¨è¿¹ï¼š

```python
def plan_joint_trajectory(start_angles, end_angles, duration, freq=50):
    """è§„åˆ’å…³èŠ‚ç©ºé—´è½¨è¿¹"""
    num_steps = int(duration * freq)
    trajectory = []
    
    for i in range(num_steps + 1):
        t = i / num_steps
        # ä½¿ç”¨ä¸‰æ¬¡å¤šé¡¹å¼æ’å€¼
        s = 3 * t**2 - 2 * t**3  # Sæ›²çº¿
        angles = start_angles + s * (end_angles - start_angles)
        trajectory.append(angles)
    
    return np.array(trajectory)

# è§„åˆ’è½¨è¿¹
start = obs["qpos"]
goal = np.array([0.0, -0.5, 0.0, 1.57, 0.0, 1.0, 0.0])
trajectory = plan_joint_trajectory(start, goal, duration=3.0)

# æ‰§è¡Œè½¨è¿¹
env = dv.make_env("airbot_play", control_mode="pd_joint_pos")
obs = env.reset()

for waypoint in trajectory:
    obs, reward, done, info = env.step(waypoint)
    env.render()
    
    if done:
        break
```

### ç¬›å¡å°”ç©ºé—´è½¨è¿¹

åœ¨ç¬›å¡å°”ç©ºé—´ä¸­è§„åˆ’ç›´çº¿è½¨è¿¹ï¼š

```python
def plan_cartesian_trajectory(start_pose, end_pose, duration, freq=50):
    """è§„åˆ’ç¬›å¡å°”ç©ºé—´ç›´çº¿è½¨è¿¹"""
    num_steps = int(duration * freq)
    trajectory = []
    
    start_pos, start_quat = start_pose[:3], start_pose[3:]
    end_pos, end_quat = end_pose[:3], end_pose[3:]
    
    for i in range(num_steps + 1):
        t = i / num_steps
        
        # ä½ç½®çº¿æ€§æ’å€¼
        pos = start_pos + t * (end_pos - start_pos)
        
        # å››å…ƒæ•°çƒé¢çº¿æ€§æ’å€¼ (SLERP)
        quat = slerp(start_quat, end_quat, t)
        
        pose = np.concatenate([pos, quat])
        trajectory.append(pose)
    
    return np.array(trajectory)

def slerp(q1, q2, t):
    """å››å…ƒæ•°çƒé¢çº¿æ€§æ’å€¼"""
    q1, q2 = np.array(q1), np.array(q2)
    
    # ç¡®ä¿é€‰æ‹©æœ€çŸ­è·¯å¾„
    if np.dot(q1, q2) < 0:
        q2 = -q2
    
    dot = np.clip(np.dot(q1, q2), -1.0, 1.0)
    omega = np.arccos(np.abs(dot))
    
    if np.sin(omega) < 1e-6:
        return q1  # å››å…ƒæ•°å‡ ä¹ç›¸åŒ
    
    return (np.sin((1-t)*omega) * q1 + np.sin(t*omega) * q2) / np.sin(omega)

# æ‰§è¡Œç¬›å¡å°”è½¨è¿¹
env = dv.make_env("airbot_play", control_mode="pd_ee_pose")
obs = env.reset()

start_pose = obs["ee_pose"]
end_pose = np.array([0.5, 0.2, 0.4, 0.0, 0.0, 0.0, 1.0])
cart_trajectory = plan_cartesian_trajectory(start_pose, end_pose, duration=2.0)

for waypoint in cart_trajectory:
    obs, reward, done, info = env.step(waypoint)
    env.render()
```

## ğŸ¯ é«˜çº§æ§åˆ¶æŠ€æœ¯

### é˜»æŠ—æ§åˆ¶

å®ç°åŸºäºåŠ›çš„æŸ”é¡ºæ§åˆ¶ï¼š

```python
def impedance_control(target_pose, current_pose, current_vel, 
                     stiffness=1000, damping=50):
    """é˜»æŠ—æ§åˆ¶å™¨"""
    # ä½ç½®è¯¯å·®
    pos_error = target_pose[:3] - current_pose[:3]
    
    # æ–¹å‘è¯¯å·® (ç®€åŒ–ä¸ºè§’åº¦è¯¯å·®)
    orient_error = orientation_error(target_pose[3:], current_pose[3:])
    
    # åˆå¹¶ä½ç½®å’Œæ–¹å‘è¯¯å·®
    pose_error = np.concatenate([pos_error, orient_error])
    
    # è®¡ç®—æœŸæœ›åŠ›/åŠ›çŸ©
    desired_wrench = stiffness * pose_error - damping * current_vel
    
    return desired_wrench

def orientation_error(target_quat, current_quat):
    """è®¡ç®—å››å…ƒæ•°æ–¹å‘è¯¯å·®"""
    # è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
    target_rot = quaternion_to_rotation_matrix(target_quat)
    current_rot = quaternion_to_rotation_matrix(current_quat)
    
    # è¯¯å·®æ—‹è½¬çŸ©é˜µ
    error_rot = target_rot @ current_rot.T
    
    # è½¬æ¢ä¸ºè½´è§’è¡¨ç¤º
    return rotation_matrix_to_axis_angle(error_rot)

# ä½¿ç”¨é˜»æŠ—æ§åˆ¶
env = dv.make_env("airbot_play", control_mode="pd_ee_pose")
obs = env.reset()

target_pose = np.array([0.5, 0.0, 0.3, 0.0, 0.0, 0.0, 1.0])

for step in range(200):
    current_pose = obs["ee_pose"]
    current_vel = obs["ee_vel"]  # å¦‚æœå¯ç”¨
    
    # è®¡ç®—é˜»æŠ—æ§åˆ¶
    wrench = impedance_control(target_pose, current_pose, current_vel)
    
    # è¿™é‡Œéœ€è¦è½¬æ¢ä¸ºå…³èŠ‚ç©ºé—´æ§åˆ¶
    # å®é™…å®ç°ä¸­ä¼šä½¿ç”¨é›…å¯æ¯”çŸ©é˜µè½¬æ¢
    
    obs, reward, done, info = env.step(target_pose)
    env.render()
```

### åŠ›æ§åˆ¶

å®ç°åŸºäºåŠ›åé¦ˆçš„æ§åˆ¶ï¼š

```python
def force_control(target_force, current_force, dt=0.02):
    """ç®€å•çš„åŠ›æ§åˆ¶å™¨"""
    force_error = target_force - current_force
    
    # PIDæ§åˆ¶
    kp, ki, kd = 0.1, 0.01, 0.05
    
    # è¿™é‡Œåº”è¯¥ç»´æŠ¤ç§¯åˆ†å’Œå¾®åˆ†é¡¹çš„å†å²
    integral_term = 0  # éœ€è¦ç´¯ç§¯
    derivative_term = 0  # éœ€è¦è®¡ç®—å˜åŒ–ç‡
    
    control_output = (kp * force_error + 
                     ki * integral_term + 
                     kd * derivative_term)
    
    return control_output

# åœ¨æœ‰åŠ›ä¼ æ„Ÿå™¨çš„ç¯å¢ƒä¸­ä½¿ç”¨
env = dv.make_env("airbot_play_force_sensor")
obs = env.reset()

target_force = np.array([0, 0, -10])  # å‘ä¸‹10Nçš„åŠ›

for step in range(100):
    if "force_sensor" in obs:
        current_force = obs["force_sensor"]
        force_cmd = force_control(target_force, current_force)
        
        # å°†åŠ›æŒ‡ä»¤è½¬æ¢ä¸ºä½ç½®æŒ‡ä»¤
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„è½¬æ¢
        
    obs, reward, done, info = env.step(action)
    env.render()
```

## ğŸ”„ æ§åˆ¶å¾ªç¯ä¼˜åŒ–

### PIDå‚æ•°è°ƒä¼˜

```python
class PIDController:
    def __init__(self, kp, ki, kd, dt=0.02):
        self.kp, self.ki, self.kd = kp, ki, kd
        self.dt = dt
        self.integral = 0
        self.previous_error = 0
    
    def update(self, error):
        # æ¯”ä¾‹é¡¹
        proportional = self.kp * error
        
        # ç§¯åˆ†é¡¹
        self.integral += error * self.dt
        integral = self.ki * self.integral
        
        # å¾®åˆ†é¡¹
        derivative = self.kd * (error - self.previous_error) / self.dt
        self.previous_error = error
        
        # PIDè¾“å‡º
        output = proportional + integral + derivative
        return output
    
    def reset(self):
        self.integral = 0
        self.previous_error = 0

# ä¸ºæ¯ä¸ªå…³èŠ‚åˆ›å»ºPIDæ§åˆ¶å™¨
joint_controllers = [
    PIDController(kp=100, ki=1, kd=10) for _ in range(7)
]

env = dv.make_env("airbot_play", control_mode="pd_joint_vel")
obs = env.reset()

target_positions = np.array([0.0, -0.5, 0.0, 1.57, 0.0, 1.0, 0.0])

for step in range(200):
    current_positions = obs["qpos"]
    joint_velocities = []
    
    for i, controller in enumerate(joint_controllers):
        error = target_positions[i] - current_positions[i]
        velocity = controller.update(error)
        joint_velocities.append(velocity)
    
    obs, reward, done, info = env.step(np.array(joint_velocities))
    env.render()
```

### å®‰å…¨é™åˆ¶

```python
class SafetyWrapper:
    def __init__(self, env, joint_limits, velocity_limits, acceleration_limits):
        self.env = env
        self.joint_limits = joint_limits  # [(min, max), ...]
        self.velocity_limits = velocity_limits
        self.acceleration_limits = acceleration_limits
        self.previous_action = None
        self.previous_time = None
    
    def step(self, action):
        # å…³èŠ‚ä½ç½®é™åˆ¶
        for i, (min_pos, max_pos) in enumerate(self.joint_limits):
            action[i] = np.clip(action[i], min_pos, max_pos)
        
        # é€Ÿåº¦é™åˆ¶
        if self.previous_action is not None:
            dt = 0.02  # å‡è®¾20msæ§åˆ¶å‘¨æœŸ
            velocity = (action - self.previous_action) / dt
            
            for i, max_vel in enumerate(self.velocity_limits):
                if abs(velocity[i]) > max_vel:
                    # é™åˆ¶é€Ÿåº¦
                    sign = np.sign(velocity[i])
                    action[i] = self.previous_action[i] + sign * max_vel * dt
        
        self.previous_action = action.copy()
        return self.env.step(action)
    
    def reset(self):
        self.previous_action = None
        return self.env.reset()

# ä½¿ç”¨å®‰å…¨åŒ…è£…å™¨
base_env = dv.make_env("airbot_play", control_mode="pd_joint_pos")
safe_env = SafetyWrapper(
    base_env,
    joint_limits=[(-3.14, 3.14)] * 7,  # å…³èŠ‚è§’åº¦é™åˆ¶
    velocity_limits=[2.0] * 7,         # é€Ÿåº¦é™åˆ¶ (rad/s)
    acceleration_limits=[10.0] * 7     # åŠ é€Ÿåº¦é™åˆ¶ (rad/sÂ²)
)
```

## ğŸ“Š æ§åˆ¶æ€§èƒ½è¯„ä¼°

### è½¨è¿¹è·Ÿè¸ªç²¾åº¦

```python
def evaluate_tracking_performance(target_trajectory, actual_trajectory):
    """è¯„ä¼°è½¨è¿¹è·Ÿè¸ªæ€§èƒ½"""
    # ä½ç½®è¯¯å·®
    position_errors = np.linalg.norm(
        target_trajectory - actual_trajectory, axis=1
    )
    
    # ç»Ÿè®¡æŒ‡æ ‡
    mae = np.mean(position_errors)  # å¹³å‡ç»å¯¹è¯¯å·®
    rmse = np.sqrt(np.mean(position_errors**2))  # å‡æ–¹æ ¹è¯¯å·®
    max_error = np.max(position_errors)  # æœ€å¤§è¯¯å·®
    
    return {
        'MAE': mae,
        'RMSE': rmse,
        'Max Error': max_error,
        'Error Std': np.std(position_errors)
    }

# æ€§èƒ½æµ‹è¯•
target_traj = plan_joint_trajectory(start, goal, duration=3.0)
actual_traj = []

env = dv.make_env("airbot_play", control_mode="pd_joint_pos")
obs = env.reset()

for waypoint in target_traj:
    obs, reward, done, info = env.step(waypoint)
    actual_traj.append(obs["qpos"])

actual_traj = np.array(actual_traj)
performance = evaluate_tracking_performance(target_traj, actual_traj)

print("è½¨è¿¹è·Ÿè¸ªæ€§èƒ½:")
for metric, value in performance.items():
    print(f"  {metric}: {value:.4f}")
```

## ğŸ¯ å®é™…åº”ç”¨ç¤ºä¾‹

### æ‹¾å–ç‰©ä½“ä»»åŠ¡

```python
def pick_object_demo():
    """æ¼”ç¤ºæ‹¾å–ç‰©ä½“çš„å®Œæ•´æ§åˆ¶æµç¨‹"""
    env = dv.make_env("airbot_play_pick_task", control_mode="pd_ee_pose")
    obs = env.reset()
    
    # 1. ç§»åŠ¨åˆ°ç‰©ä½“ä¸Šæ–¹
    object_pos = obs["object_pose"][:3]
    pre_grasp_pose = object_pos + [0, 0, 0.1]  # ä¸Šæ–¹10cm
    pre_grasp_pose = np.concatenate([pre_grasp_pose, [0, 0, 0, 1]])
    
    print("ç§»åŠ¨åˆ°é¢„æŠ“å–ä½ç½®...")
    for _ in range(50):
        obs, reward, done, info = env.step(pre_grasp_pose)
        env.render()
    
    # 2. ä¸‹é™åˆ°æŠ“å–ä½ç½®
    grasp_pose = object_pos + [0, 0, 0.02]  # ç•¥é«˜äºç‰©ä½“
    grasp_pose = np.concatenate([grasp_pose, [0, 0, 0, 1]])
    
    print("ä¸‹é™åˆ°æŠ“å–ä½ç½®...")
    for _ in range(30):
        obs, reward, done, info = env.step(grasp_pose)
        env.render()
    
    # 3. é—­åˆå¤¹çˆª
    print("é—­åˆå¤¹çˆª...")
    # è¿™é‡Œéœ€è¦å¤¹çˆªæ§åˆ¶æ¥å£
    
    # 4. æå‡ç‰©ä½“
    lift_pose = grasp_pose.copy()
    lift_pose[2] += 0.2  # æå‡20cm
    
    print("æå‡ç‰©ä½“...")
    for _ in range(50):
        obs, reward, done, info = env.step(lift_pose)
        env.render()
    
    print("æ‹¾å–ä»»åŠ¡å®Œæˆ!")

# è¿è¡Œæ¼”ç¤º
pick_object_demo()
```

## ğŸ¯ ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†æœºå™¨äººæ§åˆ¶çš„åŸºç¡€ï¼Œæ¥ä¸‹æ¥å¯ä»¥å­¦ä¹ ï¼š

ğŸ‘‰ [ä¼ æ„Ÿå™¨é…ç½®](/docs/tutorials/sensors/overview)

æˆ–è€…æ¢ç´¢æ›´é«˜çº§çš„ä¸»é¢˜ï¼š
- [æ¨¡ä»¿å­¦ä¹ ](/docs/tutorials/imitation-learning/overview)
- [å¼ºåŒ–å­¦ä¹ ](/docs/tutorials/reinforcement-learning/overview)
- [Real2Simç®¡é“](/docs/advanced/real2sim/overview) 